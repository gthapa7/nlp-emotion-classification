{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66be706e",
   "metadata": {},
   "source": [
    "# Model 2: RNN (LSTM)\n",
    "\n",
    "This notebook follows the Course 3 workflow:\n",
    "Tokenized Text Sequences → Embedding → LSTM → Dense → Softmax\n",
    "\n",
    "Steps:\n",
    "1. Load preprocessed sequences\n",
    "2. Define embedding + RNN model\n",
    "3. Train the model\n",
    "4. Evaluate on test set\n",
    "5. Save metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177a6d07",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97db7cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (16000, 3)\n",
      "Test shape: (2000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "      <td>i am feeling grouchy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion  \\\n",
       "0                            i didnt feel humiliated  sadness   \n",
       "1  i can go from feeling so hopeless to so damned...  sadness   \n",
       "2   im grabbing a minute to post i feel greedy wrong    anger   \n",
       "3  i am ever feeling nostalgic about the fireplac...     love   \n",
       "4                               i am feeling grouchy    anger   \n",
       "\n",
       "                                          clean_text  \n",
       "0                            i didnt feel humiliated  \n",
       "1  i can go from feeling so hopeless to so damned...  \n",
       "2   im grabbing a minute to post i feel greedy wrong  \n",
       "3  i am ever feeling nostalgic about the fireplac...  \n",
       "4                               i am feeling grouchy  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "\n",
    "train_df = pd.read_csv(DATA_DIR / \"train.txt\", sep=\";\", header=None, names=[\"text\", \"emotion\"])\n",
    "test_df = pd.read_csv(DATA_DIR / \"test.txt\", sep=\";\", header=None, names=[\"text\", \"emotion\"])\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "train_df[\"clean_text\"] = train_df[\"text\"].apply(clean_text)\n",
    "test_df[\"clean_text\"] = test_df[\"text\"].apply(clean_text)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85352f",
   "metadata": {},
   "source": [
    "## 2. Tokenize and pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f28f997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size (limited): 20000\n",
      "Sequence length (95th percentile): 41\n",
      "Train sequences shape: (16000, 41)\n"
     ]
    }
   ],
   "source": [
    "max_words = 20000\n",
    "\n",
    "text_tokenizer = Tokenizer(num_words=max_words, oov_token=\"<UNK>\")\n",
    "text_tokenizer.fit_on_texts(train_df[\"clean_text\"])\n",
    "\n",
    "train_seq = text_tokenizer.texts_to_sequences(train_df[\"clean_text\"])\n",
    "test_seq = text_tokenizer.texts_to_sequences(test_df[\"clean_text\"])\n",
    "\n",
    "seq_lengths = np.array([len(seq) for seq in train_seq])\n",
    "max_len = int(np.percentile(seq_lengths, 95))\n",
    "\n",
    "X_train = pad_sequences(train_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "X_test = pad_sequences(test_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_df[\"emotion\"])\n",
    "y_test = label_encoder.transform(test_df[\"emotion\"])\n",
    "class_names = list(label_encoder.classes_)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "print(f\"Vocab size (limited): {max_words}\")\n",
    "print(f\"Sequence length (95th percentile): {max_len}\")\n",
    "print(f\"Train sequences shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb28c805",
   "metadata": {},
   "source": [
    "## 3. Define embedding + LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ba7ba73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">34,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │     \u001b[38;5;34m2,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m34,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m390\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │            \u001b[38;5;34m42\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,034,480</span> (7.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,034,480\u001b[0m (7.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,034,480</span> (7.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,034,480\u001b[0m (7.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "lstm_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Embedding(input_dim=max_words, output_dim=embedding_dim),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "        tf.keras.layers.Dense(6, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "lstm_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "lstm_model.build((None, max_len))\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456d9722",
   "metadata": {},
   "source": [
    "## 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2882b6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - accuracy: 0.5284 - loss: 1.2513 - val_accuracy: 0.7925 - val_loss: 0.6334\n",
      "Epoch 2/10\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.8692 - loss: 0.4025 - val_accuracy: 0.8687 - val_loss: 0.3742\n",
      "Epoch 3/10\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.9413 - loss: 0.1790 - val_accuracy: 0.8819 - val_loss: 0.3224\n",
      "Epoch 4/10\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.9657 - loss: 0.1028 - val_accuracy: 0.8881 - val_loss: 0.3681\n",
      "Epoch 5/10\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.9762 - loss: 0.0717 - val_accuracy: 0.8925 - val_loss: 0.3807\n"
     ]
    }
   ],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train,\n",
    "    y_train_cat,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ea1d27",
   "metadata": {},
   "source": [
    "## 5. Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a04d758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Accuracy: 0.8795\n",
      "Precision (macro): 0.8155\n",
      "Recall (macro): 0.8389\n",
      "F1 (macro): 0.8201\n",
      "\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.86      0.84      0.85       275\n",
      "        fear       0.91      0.78      0.84       224\n",
      "         joy       0.91      0.94      0.92       695\n",
      "        love       0.75      0.73      0.74       159\n",
      "     sadness       0.94      0.91      0.93       581\n",
      "    surprise       0.52      0.83      0.64        66\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.82      0.84      0.82      2000\n",
      "weighted avg       0.89      0.88      0.88      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "\n",
    "lstm_probs = lstm_model.predict(X_test)\n",
    "lstm_pred = lstm_probs.argmax(axis=1)\n",
    "\n",
    "lstm_accuracy = accuracy_score(y_test, lstm_pred)\n",
    "lstm_precision, lstm_recall, lstm_f1, _ = precision_recall_fscore_support(\n",
    "    y_test, lstm_pred, average=\"macro\", zero_division=0\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {lstm_accuracy:.4f}\")\n",
    "print(f\"Precision (macro): {lstm_precision:.4f}\")\n",
    "print(f\"Recall (macro): {lstm_recall:.4f}\")\n",
    "print(f\"F1 (macro): {lstm_f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report:\\n\")\n",
    "print(classification_report(y_test, lstm_pred, target_names=class_names, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a520ee1c",
   "metadata": {},
   "source": [
    "## 6. Save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e777e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metrics to: ../results/metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN_LSTM</td>\n",
       "      <td>0.8795</td>\n",
       "      <td>0.8155</td>\n",
       "      <td>0.8389</td>\n",
       "      <td>0.8201</td>\n",
       "      <td>2026-02-07T14:24:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  accuracy  precision  recall      f1            timestamp\n",
       "0  RNN_LSTM    0.8795     0.8155  0.8389  0.8201  2026-02-07T14:24:28"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "results_dir = Path(\"../results\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "metrics_path = results_dir / \"metrics.csv\"\n",
    "\n",
    "row = {\n",
    "    \"model\": \"RNN_LSTM\",\n",
    "    \"accuracy\": round(lstm_accuracy, 4),\n",
    "    \"precision\": round(lstm_precision, 4),\n",
    "    \"recall\": round(lstm_recall, 4),\n",
    "    \"f1\": round(lstm_f1, 4),\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame([row])\n",
    "\n",
    "if metrics_path.exists():\n",
    "    metrics_df.to_csv(metrics_path, mode=\"a\", header=False, index=False)\n",
    "else:\n",
    "    metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "print(f\"Saved metrics to: {metrics_path}\")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a58eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "val_path = Path(\"../data/validation.txt\")\n",
    "if val_path.exists():\n",
    "    val_df = pd.read_csv(val_path, sep=\";\", header=None, names=[\"text\", \"emotion\"])\n",
    "    val_df[\"clean_text\"] = val_df[\"text\"].apply(clean_text)\n",
    "    val_seq = text_tokenizer.texts_to_sequences(val_df[\"clean_text\"])\n",
    "    X_val = pad_sequences(val_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "    y_val = label_encoder.transform(val_df[\"emotion\"])\n",
    "    val_probs = lstm_model.predict(X_val)\n",
    "    val_pred = val_probs.argmax(axis=1)\n",
    "    val_accuracy = accuracy_score(y_val, val_pred)\n",
    "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(\n",
    "        y_val, val_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "\n",
    "    print(\"Validation metrics (RNN):\")\n",
    "    print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Precision (macro): {val_precision:.4f}\")\n",
    "    print(f\"Recall (macro): {val_recall:.4f}\")\n",
    "    print(f\"F1 (macro): {val_f1:.4f}\")\n",
    "\n",
    "    val_row = {\n",
    "        \"model\": \"RNN_LSTM_VAL\",\n",
    "        \"accuracy\": round(val_accuracy, 4),\n",
    "        \"precision\": round(val_precision, 4),\n",
    "        \"recall\": round(val_recall, 4),\n",
    "        \"f1\": round(val_f1, 4),\n",
    "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    }\n",
    "\n",
    "    val_metrics_df = pd.DataFrame([val_row])\n",
    "    if metrics_path.exists():\n",
    "        val_metrics_df.to_csv(metrics_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        val_metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "    val_metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
